{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DXYiE1k8YZ3"
      },
      "outputs": [],
      "source": [
        "# standard imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Reshape\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import zipfile\n",
        "import keras\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiGJ46wF8m3j",
        "outputId": "f57774a3-2208-4550-8698-d4cf83a5cb28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "ENVIRONMENT: Google Drive\n"
          ]
        }
      ],
      "source": [
        "# get data #complete\n",
        "\n",
        "# connect to environment (if mounting to Google Drive)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount = True)\n",
        "    print(\"ENVIRONMENT: Google Drive\")\n",
        "except:\n",
        "    print(\"ENVIRONMENT: Local\")\n",
        "\n",
        "# first data set\n",
        "\n",
        "zipped_file = zipfile.ZipFile('/content/drive/My Drive/archive.zip', 'r') #data on drive not on collab vm, within drive.\n",
        "zipped_file.extractall('/content/') #pull drive data and put it in contents within collab vm. better than streaming from drive\n",
        "zipped_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2E4EKhCWEC5"
      },
      "outputs": [],
      "source": [
        "#second data set. \n",
        "\n",
        "zipped_file = zipfile.ZipFile('/content/drive/MyDrive/asl_alphabet_train (1).zip', 'r') #data on drive not on collab vm, within drive.\n",
        "zipped_file.extractall('/content/') #pull drive data and put it in contents within collab vm. better than streaming from drive\n",
        "zipped_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPgBv-ic8yAY"
      },
      "outputs": [],
      "source": [
        "# instantiate generators  # in progress\n",
        "# training generator/augmentation\n",
        "train_datagen = ImageDataGenerator(samplewise_center=True, \n",
        "                                   samplewise_std_normalization=True, \n",
        "                                   rotation_range = 25,\n",
        "                                   width_shift_range = 0.1,\n",
        "                                   height_shift_range = 0.1,\n",
        "                                   shear_range = 0.1,\n",
        "                                   zoom_range = 0.1,\n",
        "                                   fill_mode = 'nearest',\n",
        "                                   validation_split = .1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove unwanted classes\n",
        "!rmdir /content/asl_alphabet_train/.ipynb_checkpoints\n",
        "!rm -r /content/asl_alphabet_train/del\n",
        "!rm -r /content/asl_alphabet_train/nothing\n",
        "!rmdir /content/Training_Set/.ipynb_checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0Lqwfx9a8wx",
        "outputId": "383bf2e7-8b9c-4f69-8264-e7c26bca46c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmdir: failed to remove '/content/asl_alphabet_train/.ipynb_checkpoints': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combine data sets\n",
        "%cd asl_alphabet_train\n",
        "!for file in *; do mv -- \"$file\"/* /content/Training_Set/\"$file\"; done"
      ],
      "metadata": {
        "id": "pyNtbkNDsUD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c049b5d0-5f56-4513-c117-a5b3ee373142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/asl_alphabet_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YebcRnNi8z2_",
        "outputId": "ea1ad858-f839-404a-e635-977cc206c786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 135147 images belonging to 27 classes.\n",
            "Found 15004 images belonging to 27 classes.\n"
          ]
        }
      ],
      "source": [
        "# connect generators to data\n",
        "\n",
        "# training\n",
        "train_gen = train_datagen.flow_from_directory(directory = '/content/Training_Set/',\n",
        "                                                    subset='training',\n",
        "                                                    target_size = (64, 64),\n",
        "                                                    batch_size = 64,\n",
        "                                                    class_mode = 'categorical')\n",
        "\n",
        "# validation\n",
        "val_gen = train_datagen.flow_from_directory(directory = '/content/Training_Set/', \n",
        "                                            batch_size=64, \n",
        "                                            subset='validation',\n",
        "                                            target_size = (64, 64), \n",
        "                                            class_mode = 'categorical')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_gen.class_indices)\n"
      ],
      "metadata": {
        "id": "tRGgRYYUMbHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79v0o89Ed747",
        "outputId": "e8fcd6bf-e788-40ce-9c09-68e9ae6f2974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 64, 64, 3)\n"
          ]
        }
      ],
      "source": [
        "print(train_gen[0][0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5_1hgegcLFo"
      },
      "outputs": [],
      "source": [
        "# callbacks\n",
        "\n",
        "callback_list = []\n",
        "\n",
        "es_callback = EarlyStopping(monitor = 'val_acc', \n",
        "                           min_delta = .001, # after each epoch we want to see the val accuracy imporve by 0.001\n",
        "                           patience = 20, # if min_delta not seen after 10 epochs, stop training\n",
        "                           verbose = 1,\n",
        "                           restore_best_weights = True) # restore weights of peak val accuracy of that epoch\n",
        "\n",
        "loss_plat = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', # dont use this when using 'adam' optimizer\n",
        "                                 factor = 0.1,\n",
        "                                 patience = 20)\n",
        "\n",
        "# Add model check point, to save the best model during training: https://keras.io/api/callbacks/model_checkpoint/ was used.\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "                                filepath='/content/drive/My Drive/checkpoint/model.h5',\n",
        "                                save_weights_only = False,\n",
        "                                monitor='val_acc',\n",
        "                                mode='max',\n",
        "                                save_best_only=True)\n",
        "\n",
        "callback_list.append(es_callback)\n",
        "callback_list.append(model_checkpoint_callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVqBCl_u82lH",
        "outputId": "6c21a214-79c4-4238-ed8d-30a5938e6e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 61, 61, 16)        784       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 29, 29, 64)        16448     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 29, 29, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 13, 13, 128)       131200    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 21632)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                1384512   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 27)                1755      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,534,699\n",
            "Trainable params: 1,534,699\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# define model\n",
        "# Used https://www.kaggle.com/code/dansbecker/running-kaggle-kernels-with-a-gpu for how to use strides, and a kernel size of 4.\n",
        "model = Sequential()   # look into kernel size\n",
        "model.add(Conv2D(16, kernel_size = 4, activation = 'relu', strides=1, input_shape = (64,64,3))) #relu standard for CNN. input shape is 64 x 64 image with depth of 3 = color\n",
        "model.add(Conv2D(64, kernel_size= 4, strides=2, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(128, kernel_size=4, strides=2, activation='relu'))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(64, activation='relu')) \n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units = 27, activation = 'softmax')) # 27 options for classifcation. Use softmax for multi-class classification \n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9jID4NO85FP"
      },
      "outputs": [],
      "source": [
        "# compile model\n",
        "\n",
        "#model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
        "\n",
        "MODEL_DIR = \"/content/drive/My Drive/checkpoint/model_v8.h5\"\n",
        "reloaded = tf.keras.models.load_model(MODEL_DIR) # used to continue training after disconnections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soNFFkGN86sn",
        "outputId": "e9b84b79-8254-4ab2-974d-9a0ce2dfc59d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "2112/2112 [==============================] - 460s 218ms/step - loss: 0.5188 - acc: 0.8319 - val_loss: 0.9566 - val_acc: 0.7520\n",
            "Epoch 2/1000\n",
            "2112/2112 [==============================] - 392s 185ms/step - loss: 0.5021 - acc: 0.8352 - val_loss: 0.8428 - val_acc: 0.7578\n",
            "Epoch 3/1000\n",
            "2112/2112 [==============================] - 376s 178ms/step - loss: 0.5026 - acc: 0.8361 - val_loss: 0.8880 - val_acc: 0.7463\n",
            "Epoch 4/1000\n",
            "2112/2112 [==============================] - 378s 179ms/step - loss: 0.5028 - acc: 0.8365 - val_loss: 0.9073 - val_acc: 0.7617\n",
            "Epoch 5/1000\n",
            "2112/2112 [==============================] - 377s 179ms/step - loss: 0.4957 - acc: 0.8387 - val_loss: 0.9183 - val_acc: 0.7570\n",
            "Epoch 6/1000\n",
            "2112/2112 [==============================] - 380s 180ms/step - loss: 0.4882 - acc: 0.8423 - val_loss: 0.8005 - val_acc: 0.7713\n",
            "Epoch 7/1000\n",
            "2112/2112 [==============================] - 383s 181ms/step - loss: 0.4859 - acc: 0.8423 - val_loss: 0.9310 - val_acc: 0.7652\n",
            "Epoch 8/1000\n",
            "2112/2112 [==============================] - 404s 191ms/step - loss: 0.4755 - acc: 0.8460 - val_loss: 0.8309 - val_acc: 0.7687\n",
            "Epoch 9/1000\n",
            "2112/2112 [==============================] - 376s 178ms/step - loss: 0.4809 - acc: 0.8460 - val_loss: 0.8557 - val_acc: 0.7631\n",
            "Epoch 10/1000\n",
            "2112/2112 [==============================] - 382s 181ms/step - loss: 0.4765 - acc: 0.8478 - val_loss: 0.9028 - val_acc: 0.7581\n",
            "Epoch 11/1000\n",
            " 144/2112 [=>............................] - ETA: 5:37 - loss: 0.4654 - acc: 0.8474"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "\n",
        "history = reloaded.fit(\n",
        "    train_gen,\n",
        "    epochs=1000,\n",
        "    validation_data=val_gen,\n",
        "    callbacks = callback_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acX5ZQzR88-a"
      },
      "outputs": [],
      "source": [
        "# plot results\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training Acc.')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation Acc.')\n",
        "plt.title('Training vs. Validation Acc.')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ0ysnnWdhZY"
      },
      "outputs": [],
      "source": [
        "#save model\n",
        "model.save('/content/drive/My Drive/SignLanguage_v8.h5')\n",
        "#path_to_model = '/content/drive/My Drive/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test data\n",
        "\n",
        "zipped_file = zipfile.ZipFile('/content/drive/MyDrive/asl_alphabet_test.zip', 'r') #data on drive not on collab vm, within drive.\n",
        "zipped_file.extractall('/content/') #pull drive data and put it in contents within collab vm. better than streaming from drive\n",
        "zipped_file.close()"
      ],
      "metadata": {
        "id": "Hz_6D2ezTLgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reformat test dirs, remove unwanted classes\n",
        "%cd asl_alphabet_test\n",
        "!for file in *; do mkdir \"${file:0:1}\"; mv -- \"$file\" \"${file:0:1}\"; done\n",
        "!rmdir /content/asl_alphabet_test/.ipynb_checkpoints\n",
        "!rm -r /content/asl_alphabet_test/n"
      ],
      "metadata": {
        "id": "7iyj-KZMTOwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate\n",
        "model_path = '/content/drive/My Drive/checkpoint/model_v8.2.h5'\n",
        "reconstructed_model = keras.models.load_model(model_path)\n",
        "\n",
        "test_datagen = ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)\n",
        "\n",
        "test_gen = test_datagen.flow_from_directory(directory = '/content/asl_alphabet_test', \n",
        "                                            target_size = (64, 64), \n",
        "                                            class_mode = 'categorical')                             \n",
        "\n",
        "reconstructed_model.evaluate(test_gen)"
      ],
      "metadata": {
        "id": "l76_HQr8TSkA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "SignLanguage_V8.2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}