{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CS485 HW2 AY22-2\n# Created by Jack Summers\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout #standard layers needed for CNN\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator #allow us to create a generator to point at a directory and augment data on the fly\nfrom tensorflow.keras.utils import to_categorical # for one-hot encoding labels\nimport keras","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-01T14:39:53.432149Z","iopub.execute_input":"2022-03-01T14:39:53.432698Z","iopub.status.idle":"2022-03-01T14:39:59.124464Z","shell.execute_reply.started":"2022-03-01T14:39:53.432531Z","shell.execute_reply":"2022-03-01T14:39:59.123603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load data\n\nx_train = \"../input/HW2Images/x_train.npy/x_train.npy\"\ny_train = \"../input/HW2Images/y_train.npy\"\nx_val = \"../input/HW2Images/x_val.npy/x_val.npy\"\ny_val = \"../input/HW2Images/y_val.npy\"\n\n\n# load arrays\n\nx_train = np.load(x_train)\ny_train = np.load(y_train)\nx_val = np.load(x_val)\ny_val = np.load(y_val)\n\n\n#normalize with .astype per MAJ Ruiz's advice\n\nx_train = x_train.astype('float32') / 255 #floating point values\nx_val = x_val.astype('float32') / 255\n\n#combine data, split later in generator\nx_train = np.append(x_train, x_val,axis=0) #Learned how to append at https://numpy.org/doc/stable/reference/generated/numpy.append.html\ny_train = np.append(y_train, y_val,axis=0)\ny_train = to_categorical(y_train)\n\n\n#print(len(y_train) - len(y_train)*0.2)\n#print(151284/128)\n\n#reshape vector IOT feed it into model and generator\nx_train = x_train.reshape(189106, 28, 28, 1) #learned how to re shape at https://stackoverflow.com/questions/63279168/valueerror-input-0-of-layer-sequential-is-incompatible-with-the-layer-expect\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T14:39:59.126372Z","iopub.execute_input":"2022-03-01T14:39:59.126957Z","iopub.status.idle":"2022-03-01T14:40:01.666469Z","shell.execute_reply.started":"2022-03-01T14:39:59.126917Z","shell.execute_reply":"2022-03-01T14:40:01.665292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#call backs copied from my HW1 problem set:\n\ncallback_list = []\n\nes_callback = EarlyStopping(monitor = 'val_acc', \n                           min_delta = .001, # after each epoch we want to see the val accuracy imporve by 0.001\n                           patience = 20, # if min_delta not seen after 10 epochs, stop training\n                           verbose = 1,\n                           restore_best_weights = True) # restore weights of peak val accuracy of that epoch\n\nloss_plat = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', # dont use this when using 'adam' optimizer\n                                 factor = 0.1,\n                                 patience = 20)\n\ncallback_list.append(es_callback)\n#callback_list.append(loss_plat)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T14:40:01.667793Z","iopub.execute_input":"2022-03-01T14:40:01.668046Z","iopub.status.idle":"2022-03-01T14:40:01.67575Z","shell.execute_reply.started":"2022-03-01T14:40:01.668014Z","shell.execute_reply":"2022-03-01T14:40:01.675039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generators\n\n#data augmentation\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2)\n\ntrain_datagen.fit(x_train)\n\ntrain_gen = train_datagen.flow(x_train, y_train, batch_size=128, subset='training')\nval_gen = train_datagen.flow(x_train, y_train, batch_size=37821, subset='validation') #all validation data at once\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T14:40:01.67752Z","iopub.execute_input":"2022-03-01T14:40:01.67783Z","iopub.status.idle":"2022-03-01T14:40:02.066233Z","shell.execute_reply.started":"2022-03-01T14:40:01.677798Z","shell.execute_reply":"2022-03-01T14:40:02.065218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define model\n                       \n\nmodel = Sequential()\nmodel.add(Conv2D(16, kernel_size = (3, 3), activation = 'relu', input_shape = (28,28,1))) #kernal size 3x3 standard and so is relu for CNN. input shape is 28 x 28 image with depth of 1 = grayscale\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu'))\nmodel.add(MaxPooling2D(2, 2))\n                                          \nmodel.add(Flatten())\n\nmodel.add(Dense(units = 128, activation = 'relu')) \nmodel.add(Dense(units = 8, activation = 'softmax')) # 8 options for classifcation. Use softmax for multi-class classification \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T14:40:02.067366Z","iopub.execute_input":"2022-03-01T14:40:02.067579Z","iopub.status.idle":"2022-03-01T14:40:02.217549Z","shell.execute_reply.started":"2022-03-01T14:40:02.067552Z","shell.execute_reply":"2022-03-01T14:40:02.21673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compile model\n# \"categorical crossentropy is almost always the loss function you should use for multi class classification\"\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc']) ","metadata":{"execution":{"iopub.status.busy":"2022-03-01T14:40:02.218921Z","iopub.execute_input":"2022-03-01T14:40:02.219172Z","iopub.status.idle":"2022-03-01T14:40:02.232753Z","shell.execute_reply.started":"2022-03-01T14:40:02.219139Z","shell.execute_reply":"2022-03-01T14:40:02.23188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train model with generator\n\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch= 1181, # 151284 training images / 128 batch size = 1181.90625\n    epochs=1000,\n    validation_data=val_gen,\n    validation_steps=1, #batch size / 0.2*allData = approx. 1\n    callbacks = callback_list)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T14:40:02.234669Z","iopub.execute_input":"2022-03-01T14:40:02.235116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot results\nmodel.save(\"./history.h5\") # used https://www.tensorflow.org/tutorials/keras/save_and_load to learn how to save and load models\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training Acc.')\nplt.plot(epochs, val_acc, 'b', label='Validation Acc.')\nplt.title('Training vs. Validation Acc.')\n\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training vs Validation Loss')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load saved model \n#new_model = tf.keras.models.load_model('../input/models/0.55history.h5')\n\nfile = '../input/HW2xtest/x_test.npy' #input test data\n   \n\n#load and normalize test data \ntest_data = np.load(file)\ntest_data = test_data.astype('float32') / 255\ntest_data = test_data.reshape(47280, 28, 28, 1)\n#create predictions\npredictions = model.predict(test_data)   #change if using loaded model\n\n#output to csv\nimport csv\ncount = 0\nwith open('./HW2_predictions4.csv', mode = 'w') as file:\n    row = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n    row.writerow(['Id', 'Category']) # headers\n    for item in predictions:\n        row.writerow([count, np.argmax(item)]) # add row: count, index of predicted value\n        count +=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}